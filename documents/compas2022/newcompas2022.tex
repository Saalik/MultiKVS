% -*- mode: latex; coding: utf-8-unix -*-

% ===========================================================
%                              Choix de thématique
%===========================================================
% Une des quatre options 'parallelisme', 'architecture', 'systeme' 
% 'tempsreel' doit être utilisée avec le style compas2021
\documentclass[systeme,french,english]{compas2022}
\usepackage[utf8]{inputenc}
\usepackage{comment}
\usepackage{tikz}
\usepackage{todonotes}

\usepackage{algorithm2e}
\usepackage[T1]{fontenc} %% Vector fonts
\usepackage[numbers,square,sort&compress]{natbib} %% Better bibliography
\usepackage{parskip} \parskip=1ex plus 5pt minus 1pt \parindent=4ex %% indent paragraphs
\usepackage{xspace}  %% Macros with automatic spacing
\usepackage[defblank]{paralist}  %% inline lists

\makeatletter 
\tikzstyle{inlinenotestyle} = [
    draw=\@todonotes@currentbordercolor,
    fill=\@todonotes@currentbackgroundcolor,
    line width=0.5pt,
    inner sep = 0.8 ex,
    rounded corners=4pt]

\renewcommand{\@todonotes@drawInlineNote}{%
        {\begin{tikzpicture}[remember picture,baseline=(current bounding box.base)]%
            \draw node[inlinenotestyle,font=\@todonotes@sizecommand, anchor=base,baseline]{%
              \if@todonotes@authorgiven%
                {\noindent \@todonotes@sizecommand \@todonotes@author:\,\@todonotes@text}%
              \else%
                {\noindent \@todonotes@sizecommand \@todonotes@text}%
              \fi};%
           \end{tikzpicture}}}%
\newcommand{\mytodo}[1]{\@todo[inline]{#1}}%
\makeatother


%===========================================================
%                               Title
%===========================================================

\toappear{1} % Conserver cette ligne pour la version finale

\begin{document}
\selectlanguage{english}

\title{Towards correct high-performance database backends}
\shorttitle{Correct High-Performance backends}

\author{Saalik Hatia(1), Annette Bieniusa(2), Carla Ferreira(3), Gustavo Petri(4), Marc Shapiro(1)}


\address{
  (1)~LIP6--Sorbonne-Université, Paris, France  \\
  (2)~TU Kaiserslautern, Germany \\
  (3)~Universidade NOVA de Lisboa, Portugal \\
  (4)~ARM, Cambridge, United Kingdom\\
  }

\date{\today}

\maketitle

%===========================================================         %
%R\'esum\'e
%===========================================================  
\begin{abstract}
  Our goal is to design and implement a full-featured, high-performance
  geo-distributed database backend that is correct by construction.
  We adopt a stepwise approach.
  We start from a simplistic concurrent database backend; we formalise an
  operational semantics and its key invariants; we also provide a
  reference implementation and test cases exercising the invariants.
  From there, we plan to prove that the semantic model satisfies the
  invariants (proof in Coq), and to check that the implementation respects
  the model thanks to litmus-test cases.
  Each following step adds a single feature.
  We formalise the feature, and expect to show formally and experimentally
  that the improved system simulates the simpler one.
  We expect that the features compose, so that the correctness of the full
  system with all the features follows from the individual proofs.
  At the time of writing, the operational semantics for several steps is
  complete; the translation to Coq and the reference implementation are
  ongoing.

  \MotsCles{
    système réparti,
    base de données géo-distribuée,
    vérification formelle,
    test
    }
\end{abstract}

\tableofcontents

%=========================================================
\section{Introduction}
%=========================================================

Modern online applications use a distributed database management system
(DDBMS), whose storage backend provides users with available and
consistent data.
The purpose of a backend is conceptually simple: to store and retrieve
shared data.
However, for performance and fault-tolerance reasons, the internals of a
typical database backend are very complex, with many moving parts that interact
in hard-to-understand ways.
Existing backends are designed in a manual and ad-hoc manner;
inevitably, they have bugs that impact the consistency and integrity of
data.

The goal of this work is to build a database backend that is correct by
design, while including some important optimisations (e.g., caching) and
properties (e.g., fault tolerance).
Formalising a backend that provide users with fast read and writes,
data safety and geo-replication is hard.
Any attempt to verify at once some monolithic specification that
includes all the interesting properties, is probably doomed to failure.
Instead, we propose the an incremental approach,  decomposing the
system into a set of small, orthogonal modules and features.

Our starting baseline is a bare-bones and ``obviously correct'' backend,
restricted to its simplest function, i.e., transactions reading and
writing data.
We formalise its operational semantics and specify its invariants.
We also provide a (hand-written) reference implementation, and use test
cases derived from the formal model to check that it follows the
semantics and does not violate the invariants.

Each design step adds a single feature, e.g., a cache or logging,
with its associated operational semantics and invariants.
We use simulation (or bisimulation) to show that the formal model is
equivalent without and with the feature, modulo the added constraints.
We take a similar approach with the reference implementation.
Furthermore, we compare performance without and with the feature.

Our ultimate aim is to show that the final design, including all the
features, is both correct (by simulation of the baseline) and has
comparable performance to ad-hoc database backends with similar features.
We claim that providing developers with a clear model helps to implement
a correct, high performance and geo-distributed database backend.

\section{Background}

Avant de parler d'implémentation je reparle rapidement du background.

\subsection{Transactions and timestamps}
\label{sec:transactions}


A client of the backend executes transactions.
Classically, a transaction is a sequence of reads and updates, bounded by
a begin and an end.
The transaction either aborts with no effects; or it commits and all its
commits become visible atomically to later transactions.
Formally, the choice between abort and commit is non-deterministic (in
practice it depends on the application's invariants and on external
events, such as sufficient resources being available).

We formalise the mutual ordering of transactions with abstract
\emph{timestamps}.
Committing a transaction $i$ assigns it a commit timestamp
$\mathit{Ct}_{i}$; all its updates are labelled with this commit
timestamp.
Conversely, a transaction $j$  has a \emph{snapshot timestamp} (or dependency
timestamp) $\mathit{Dt}_{j}$.
Transaction $i$ precedes transaction $j$ if $\mathit{Ct}_{i} \le
\mathit{Dt}_{j}$.
Transaction $j$ \emph{depends on} all transactions $i$ such $i$ precedes
$j$; any read that $j$ performs includes all the updates with a label
less or equal to $\mathit{Dt}_{j}$.
The set of such preceding updates is called the \emph{snapshot} of
transaction $j$.
 
Our formalisation is agnostic to the transaction model (e.g.,
serialisable or snapshot-isolation).
For space reasons, this paper considers a single model, Transactional
Causal Plus Consistency (TCC+), i.e., causal consistency with
transactions and convergence \cite{rep:syn:sh228,rep:pro:sh182}.
Under TCC+, timestamps form a partial order (consistent with
happened-before), and a snapshot must be a causally-consistent cut.
Other transaction models differ by timestamp ordering being partial or
total, and by constraints on beginning and committing a transaction.

The formal model represents a transaction with the following information:
\begin{compactitem}
\item $\tau$: unique transaction identifier.
\item $\mathit{Dt}$: dependency timestamp.
\item {$\varepsilon$}: effect map, records the updates made by the
  transaction.
  A write updates the effect map.
\item ${R}$: read set, records the objects read by the transaction.
  A Read updates the read set.
\item $\mathit{Ct}$: commit timestamp, assigned if and when the
  transaction commits.
\item $\mathit{State}$: status, either \emph{not\_started}, \emph{live} or \emph{terminated}.\\
\end{compactitem}



\subsection{Object-versions}

In order to be generic and agnostic to transaction model, our formalism
considers that each update (or more precisely, the associated commit)
creates a new version of the updated object.%
%
\footnote{
%
  This is compatible with Multi-Version Concurrency Control (MVCC), used
  by many databases to increase parallelism, but does not mandate MVCC
  in the implementation.
}
%
As mentioned above, each such version is labelled with the commit
timestamp of the corresponding transaction.
A particular version of a particular object maps the pair
$(\mathit{key},\mathit{Ct})$ to the corresponding value, which for the
purposes of this paper is an untyped ``\emph{blob}.''
In the above, $\mathit{key}$ is the unique identifier or key of the
object, and $\mathit{Ct}$ is the commit timestamp of the transaction
that writes this object-version.


\section{Unbounded Versions}

We start with an intuitive simple in-memory key-value store.
Therefore if a crash or restart occurs the database is lost.
It is also based on the assumption that the we have infinite memory, therefore we keep all Object-versions of every committed transaction in the backend.
Finally we maintain a dependency graph between transaction to determine visibility of transactions.


\subsection{Specification}

\mytodo{Here I will put labels for every new invariants and then used them in the next section to show the implementation.
Some are duplicate but it's to ensure that I have everything I will trim it down later.
Also the lookup part is missing but I will add it later.}

In this section we present the semantics from the viewpoint of the system.
The general behavior describe in this section is as follows.
A client starts a transaction and execute a sequence of operations and tries to commit or abort the transaction.
This means all operations executed by the client must be part of a \emph{live} transaction.
This also include the commit and abort call.
Another guarantee our model provide is that for any committed transaction, the commit timestamp must be greater than the dependency timestamp.

These are the transaction invariants maintained in the system:
\[
  \begin{array}{lcl}
    \emph{$Ct \neq null => Dt_i > Ct_i$}\\
    \emph{$(read, effect, commit, abort) => State = live$}
  \end{array} 
\]

\textbf{Start transaction} (Session part missing)
When a client starts a transaction, if the client specify a dependency timestamp $Dt$ the system checks if it is valid in regards to consistency guarantees and creates a transaction object.
If no dependency timestamp is specified than system assign a valid snapshot to the transaction.
(On parle de l'implémentation de ce mécanisme plus tard)
The transaction is then assigned a unique transaction identifier $\tau_i$ and $\emph{State} \leftarrow \emph{live}$.
Here are the preconditions, invariants and postconditions:
(Alt) From these requirement we extract the following preconditions:

Preconditions:\\
$State \neq live$
$Ct = null$

Invariants:\\
\emph{$Ct \neq null => Dt_i > Ct_i$}\\
$Ct = null$

Postconditions:\\
$State_i = \emph{live}$ \\
$Ct = null$


\textbf{Read} returns the correct Object-version in the store that is valid in regads to our consistency model.
If two conccurent Object-versions exist, meaning they are part of two transaction that have a commit timestamp lower or equal than the dependency timestamp and are not related by precedence.
(NOTE: première fois que j'en parle)
We use a deterministic algorithm, ie. merge or LWW, to choose the Object-version returned.
In case there are no existing Object-version in memory we return a default value.
Finally the system adds the key to the read set $R_i$ of the transaction. 

Preconditions:\\ 
$State_i = \emph{live}$ \\
$Ct = null$

Invariants:\\
$State_i = \emph{live}$ \\
$Ct = null$

Postconditions:\\
$R_i = R \cup \{key\}$ \\

\textbf{Effect} the system creates a new Object-version by merging the Object-version that is returned by \emph{read(key)} with the effect.

Preconditions:\\ 
$State_i = \emph{live}$ \\
$Ct = null$

Invariants:\\
$State_i = \emph{live}$ \\
$Ct = null$

Postconditions:\\
$\varepsilon_i = \varepsilon_i \cup \{(key, \tau_i)\}$ \\

\textbf{Commit} the system checks if the effects $\varepsilon_i$ are valid in regards to the consistency model and abort if not. (Need more precision. Maybe earlier here does not seem to be the correct section)
If the effects are valid the Objects versions are added to the Store and assigns a commit timestamp $Ct_i$.
Finally the transaction's $State$ is set to $terminated$ making the effects visible in the store atomically.

Preconditions:\\ 
$State_i = \emph{live}$ \\
$Ct = null$

Postconditions:\\
$Store = Store \cup \varepsilon_i$\\
$State = terminated$\\
$Dt_i \geq Ct_i$.\\

\textbf{Abort} updates the state of the transaction to $terminated$ without updating the Store with the effects of the transaction and no commit timestamp is assigned

Preconditions:\\ 
$State_i = \emph{live}$ \\
$Ct = null$ 

Postconditions:\\
$State = terminated$\\
$Ct = null$


\subsection{Implementation}



\section{Bounded Versions}

Now we consider a bounded version of the key-value store.
To achieve this, we choose to define an arbitrary limitation on the number of versions used by the system.\\
\mytodo{Talk about tracking the memory usage of the system in the system. The policies in case we reach the threshold. And the need for garbage collection now.
I will add which calls it affects and the new added invariants}

\subsection{Specification}

\subsection{Implementation}

\section{Comparison and testing}

In this section we present the tests that we use to show that the unbounded version simulate the unbounded version.
We also talk about the difference between the two versions.
The evolution of the number of versions, test the new invariants and the performance impact on the system.

\section{Conclusion}

\bibliography{exemple,predef,shapiro-bib-ext,shapiro-bib-perso}


\end{document}